{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORCx8i1o1ZK9",
    "tags": []
   },
   "source": [
    "# B01_LLaMA-Factory-Cmd-lora_mutiple_gpu-Singularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAnZLFDg1ZLC"
   },
   "source": [
    "## 環境安裝\n",
    "- 切換到原生 Python 3 (ipykernel)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 下載 LLaMA-Factory 套件\n",
    "!rm -rf LLaMA-Factory\n",
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT (lora in mutple GPU with singularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# 虛擬絕對路徑\n",
      "cd /DEEPSPEED/LLaMA-Factory\n",
      "\n",
      "HF_TOKEN='hf_' nohup deepspeed --num_gpus 4 src/train_bash.py \\\n",
      "    --deepspeed examples/full_multi_gpu/ds_z3_config.json \\\n",
      "    --stage sft \\\n",
      "    --do_train \\\n",
      "    --model_name_or_path /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf \\\n",
      "    --dataset alpaca_gpt4_en,glaive_toolcall \\\n",
      "    --dataset_dir data \\\n",
      "    --template default \\\n",
      "    --finetuning_type lora \\\n",
      "    --lora_target q_proj,v_proj \\\n",
      "    --output_dir ../saves/LLaMA2-7B/lora/sft \\\n",
      "    --overwrite_cache \\\n",
      "    --overwrite_output_dir \\\n",
      "    --cutoff_len 1024 \\\n",
      "    --per_device_train_batch_size 1 \\\n",
      "    --per_device_eval_batch_size 1 \\\n",
      "    --gradient_accumulation_steps 8 \\\n",
      "    --lr_scheduler_type cosine \\\n",
      "    --logging_steps 10 \\\n",
      "    --save_steps 100 \\\n",
      "    --eval_steps 100 \\\n",
      "    --evaluation_strategy steps \\\n",
      "    --load_best_model_at_end \\\n",
      "    --learning_rate 5e-5 \\\n",
      "    --num_train_epochs 3.0 \\\n",
      "    --max_samples 1000 \\\n",
      "    --val_size 0.1 \\\n",
      "    --plot_loss \\\n",
      "    --fp16 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## qlora in single GPU (bitsandbytes) \n",
    "## 啟動 Llama-Factory Cmd\n",
    "## 請記得修改最後一行, 給予huggingface token,  HF_TOKEN='hf_' ?\n",
    "## 記錄檔案為 llamafactory.log \n",
    "## 量化請加上     --quantization_bit 4 \\\n",
    "\n",
    "cat << \\\\EOF >  demo.cmd\n",
    "#!/bin/bash\n",
    "\n",
    "# 虛擬絕對路徑\n",
    "cd /DEEPSPEED/LLaMA-Factory\n",
    "\n",
    "HF_TOKEN='hf_' nohup deepspeed --num_gpus 4 src/train_bash.py \\\n",
    "    --deepspeed examples/full_multi_gpu/ds_z3_config.json \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf \\\n",
    "    --dataset alpaca_gpt4_en,glaive_toolcall \\\n",
    "    --dataset_dir data \\\n",
    "    --template default \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --output_dir ../saves/LLaMA2-7B/lora/sft \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --eval_steps 100 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 1000 \\\n",
    "    --val_size 0.1 \\\n",
    "    --plot_loss \\\n",
    "    --fp16 \n",
    "\n",
    "\\EOF\n",
    "\n",
    "chmod 755 demo.cmd\n",
    "\n",
    "cat demo.cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-12 07:50:33,683] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-12 07:50:35,788] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "Detected CUDA_VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.\n",
      "[2024-03-12 07:50:35,788] [INFO] [runner.py:568:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None src/train_bash.py --deepspeed examples/full_multi_gpu/ds_z3_config.json --stage sft --do_train --model_name_or_path /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf --dataset alpaca_gpt4_en,glaive_toolcall --dataset_dir data --template default --finetuning_type lora --lora_target q_proj,v_proj --output_dir ../saves/LLaMA2-7B/lora/sft --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 10 --save_steps 100 --eval_steps 100 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 3.0 --max_samples 1000 --val_size 0.1 --plot_loss --fp16\n",
      "[2024-03-12 07:50:37,840] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-12 07:50:38,917] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.2-1+cuda11.8\n",
      "[2024-03-12 07:50:38,918] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.2-1\n",
      "[2024-03-12 07:50:38,918] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.2-1\n",
      "[2024-03-12 07:50:38,918] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "[2024-03-12 07:50:38,918] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.2-1+cuda11.8\n",
      "[2024-03-12 07:50:38,918] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "[2024-03-12 07:50:38,918] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.2-1\n",
      "[2024-03-12 07:50:38,918] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2024-03-12 07:50:38,918] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2024-03-12 07:50:38,918] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2024-03-12 07:50:38,918] [INFO] [launch.py:163:main] dist_world_size=4\n",
      "[2024-03-12 07:50:38,918] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "[2024-03-12 07:50:43,471] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-12 07:50:43,471] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-12 07:50:43,475] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-12 07:50:43,476] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-12 07:50:47,306] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-03-12 07:50:47,306] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-03-12 07:50:47,306] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-03-12 07:50:47,306] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-03-12 07:50:47,306] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "03/12/2024 07:50:47 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "03/12/2024 07:50:47 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n",
      "03/12/2024 07:50:47 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "03/12/2024 07:50:47 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n",
      "03/12/2024 07:50:47 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "03/12/2024 07:50:47 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n",
      "03/12/2024 07:50:47 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "03/12/2024 07:50:47 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2044] 2024-03-12 07:50:47,439 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2044] 2024-03-12 07:50:47,439 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2044] 2024-03-12 07:50:47,439 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2044] 2024-03-12 07:50:47,439 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2044] 2024-03-12 07:50:47,439 >> loading file tokenizer.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/12/2024 07:50:47 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "03/12/2024 07:50:47 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "03/12/2024 07:50:47 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "03/12/2024 07:50:47 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "03/12/2024 07:50:47 - INFO - llmtuner.data.loader - Loading dataset alpaca_gpt4_data_en.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 52002 examples [00:00, 133625.15 examples/s]\n",
      "Converting format of dataset: 100%|██████████| 1000/1000 [00:00<00:00, 63599.21 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/12/2024 07:50:49 - INFO - llmtuner.data.loader - Loading dataset glaive_toolcall_10k.json...\n",
      "03/12/2024 07:50:49 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/glaive_toolcall_10k.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 10000 examples [00:00, 20643.13 examples/s]\n",
      "Converting format of dataset: 100%|██████████| 1000/1000 [00:00<00:00, 20198.91 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/12/2024 07:51:01 - INFO - llmtuner.data.loader - Loading dataset alpaca_gpt4_data_en.json...\n",
      "03/12/2024 07:51:01 - INFO - llmtuner.data.loader - Loading dataset alpaca_gpt4_data_en.json...\n",
      "03/12/2024 07:51:01 - INFO - llmtuner.data.loader - Loading dataset alpaca_gpt4_data_en.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting format of dataset: 100%|██████████| 1000/1000 [00:00<00:00, 50085.43 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/12/2024 07:51:02 - INFO - llmtuner.data.loader - Loading dataset glaive_toolcall_10k.json...\n",
      "03/12/2024 07:51:02 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/glaive_toolcall_10k.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting format of dataset: 100%|██████████| 1000/1000 [00:00<00:00, 48782.89 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/12/2024 07:51:03 - INFO - llmtuner.data.loader - Loading dataset glaive_toolcall_10k.json...\n",
      "03/12/2024 07:51:03 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/glaive_toolcall_10k.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting format of dataset: 100%|██████████| 1000/1000 [00:00<00:00, 10235.15 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/12/2024 07:51:03 - INFO - llmtuner.data.loader - Loading dataset glaive_toolcall_10k.json...\n",
      "03/12/2024 07:51:03 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/glaive_toolcall_10k.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting format of dataset: 100%|██████████| 1000/1000 [00:00<00:00, 19441.03 examples/s]\n",
      "Converting format of dataset: 100%|██████████| 1000/1000 [00:00<00:00, 19493.34 examples/s]\n",
      "Converting format of dataset: 100%|██████████| 1000/1000 [00:00<00:00, 19859.11 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 2000/2000 [00:05<00:00, 369.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "[12968, 29901, 25538, 2211, 25562, 363, 7952, 292, 9045, 29891, 29889, 13, 7900, 22137, 29901, 29871, 29871, 29896, 29889, 382, 271, 263, 6411, 8362, 322, 18254, 768, 2738, 652, 300, 29901, 8561, 1854, 596, 592, 1338, 526, 20978, 573, 310, 263, 12875, 310, 285, 21211, 322, 18655, 1849, 29892, 20793, 26823, 29892, 3353, 2646, 1144, 29892, 322, 9045, 29891, 285, 1446, 29889, 910, 6911, 304, 3867, 596, 3573, 411, 278, 18853, 18254, 374, 1237, 304, 740, 472, 967, 1900, 322, 508, 1371, 5557, 17168, 293, 10267, 2129, 29889, 13, 13, 29906, 29889, 2201, 482, 297, 4943, 9128, 6354, 29901, 1222, 6269, 895, 338, 7618, 1455, 363, 7344, 292, 4549, 289, 2873, 29892, 2301, 7799, 29892, 322, 5881, 29875, 586, 6151, 1070, 9045, 29889, 319, 326, 363, 472, 3203, 29871, 29896, 29945, 29900, 6233, 310, 17768, 403, 14911, 711, 293, 15058, 470, 29871, 29955, 29945, 6233, 310, 14877, 20657, 15058, 1269, 4723, 29889, 13, 13, 29941, 29889, 3617, 3307, 8709, 29901, 24162, 3307, 11029, 8709, 338, 7618, 1455, 363, 9128, 322, 19119, 1532, 29899, 915, 292, 29889, 739, 6911, 304, 1072, 5987, 286, 2092, 29892, 11157, 25323, 3321, 740, 29892, 322, 11286, 9045, 29891, 14321, 322, 5198, 1540, 740, 29889, 319, 326, 363, 29871, 29955, 29899, 29929, 6199, 310, 8709, 1269, 4646, 29889, 2]\n",
      "inputs:\n",
      "Human: Give three tips for staying healthy.\n",
      "Assistant:  1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
      "\n",
      "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 29871, 29896, 29889, 382, 271, 263, 6411, 8362, 322, 18254, 768, 2738, 652, 300, 29901, 8561, 1854, 596, 592, 1338, 526, 20978, 573, 310, 263, 12875, 310, 285, 21211, 322, 18655, 1849, 29892, 20793, 26823, 29892, 3353, 2646, 1144, 29892, 322, 9045, 29891, 285, 1446, 29889, 910, 6911, 304, 3867, 596, 3573, 411, 278, 18853, 18254, 374, 1237, 304, 740, 472, 967, 1900, 322, 508, 1371, 5557, 17168, 293, 10267, 2129, 29889, 13, 13, 29906, 29889, 2201, 482, 297, 4943, 9128, 6354, 29901, 1222, 6269, 895, 338, 7618, 1455, 363, 7344, 292, 4549, 289, 2873, 29892, 2301, 7799, 29892, 322, 5881, 29875, 586, 6151, 1070, 9045, 29889, 319, 326, 363, 472, 3203, 29871, 29896, 29945, 29900, 6233, 310, 17768, 403, 14911, 711, 293, 15058, 470, 29871, 29955, 29945, 6233, 310, 14877, 20657, 15058, 1269, 4723, 29889, 13, 13, 29941, 29889, 3617, 3307, 8709, 29901, 24162, 3307, 11029, 8709, 338, 7618, 1455, 363, 9128, 322, 19119, 1532, 29899, 915, 292, 29889, 739, 6911, 304, 1072, 5987, 286, 2092, 29892, 11157, 25323, 3321, 740, 29892, 322, 11286, 9045, 29891, 14321, 322, 5198, 1540, 740, 29889, 319, 326, 363, 29871, 29955, 29899, 29929, 6199, 310, 8709, 1269, 4646, 29889, 2]\n",
      "labels:\n",
      "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
      "\n",
      "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:726] 2024-03-12 07:51:07,269 >> loading configuration file /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-03-12 07:51:07,270 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|          | 0/2000 [00:00<?, ? examples/s][INFO|modeling_utils.py:3254] 2024-03-12 07:51:07,328 >> loading weights file /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1400] 2024-03-12 07:51:07,328 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|modeling_utils.py:3363] 2024-03-12 07:51:07,328 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "[INFO|configuration_utils.py:845] 2024-03-12 07:51:07,332 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset: 100%|██████████| 2000/2000 [00:05<00:00, 369.82 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 2000/2000 [00:05<00:00, 367.66 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 2000/2000 [00:05<00:00, 366.89 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-12 07:51:15,131] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 188672\n",
      "[2024-03-12 07:51:15,135] [INFO] [partition_parameters.py:349:__exit__] finished initializing model - num_params = 182, num_elems = 4.20B\n",
      "Error while terminating subprocess (pid=188447): \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/DEEPSPEED/LLaMA-Factory/src/train_bash.py\", line 14, in <module>\n",
      "    main()\n",
      "  File \"/DEEPSPEED/LLaMA-Factory/src/train_bash.py\", line 5, in main\n",
      "    run_exp()\n",
      "  File \"/DEEPSPEED/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 32, in run_exp\n",
      "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "  File \"/DEEPSPEED/LLaMA-Factory/src/llmtuner/train/sft/workflow.py\", line 33, in run_sft\n",
      "    model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)\n",
      "  File \"/DEEPSPEED/LLaMA-Factory/src/llmtuner/model/loader.py\", line 100, in load_model\n",
      "    model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, config=config, **init_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 561, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3375, in from_pretrained\n",
      "    model = cls(config, *model_args, **model_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 509, in wrapper\n",
      "    f(module, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1103, in __init__\n",
      "    self.model = LlamaModel(config)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 509, in wrapper\n",
      "    f(module, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 924, in __init__\n",
      "    [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 924, in <listcomp>\n",
      "    [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 509, in wrapper\n",
      "    f(module, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 699, in __init__\n",
      "    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 509, in wrapper\n",
      "    f(module, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 509, in wrapper\n",
      "    f(module, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 288, in __init__\n",
      "    self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 519, in wrapper\n",
      "    self._post_init_method(module)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1054, in _post_init_method\n",
      "    self._zero_init_param(param)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1013, in _zero_init_param\n",
      "    param.partition()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1328, in partition\n",
      "    self._partition(param_list, has_been_updated=has_been_updated)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1477, in _partition\n",
      "    self._partition_param(param, has_been_updated=has_been_updated)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1568, in _partition_param\n",
      "    param.ds_tensor.copy_(src_tensor)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/DEEPSPEED/LLaMA-Factory/src/train_bash.py\", line 14, in <module>\n",
      "    main()\n",
      "  File \"/DEEPSPEED/LLaMA-Factory/src/train_bash.py\", line 5, in main\n",
      "    run_exp()\n",
      "  File \"/DEEPSPEED/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 32, in run_exp\n",
      "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "  File \"/DEEPSPEED/LLaMA-Factory/src/llmtuner/train/sft/workflow.py\", line 33, in run_sft\n",
      "    model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)\n",
      "  File \"/DEEPSPEED/LLaMA-Factory/src/llmtuner/model/loader.py\", line 100, in load_model\n",
      "    model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, config=config, **init_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 561, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3375, in from_pretrained\n",
      "    model = cls(config, *model_args, **model_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 509, in wrapper\n",
      "    f(module, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1103, in __init__\n",
      "    self.model = LlamaModel(config)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 509, in wrapper\n",
      "    f(module, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 924, in __init__\n",
      "    [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 924, in <listcomp>\n",
      "    [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 509, in wrapper\n",
      "    f(module, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 699, in __init__\n",
      "    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 509, in wrapper\n",
      "    f(module, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 288, in __init__\n",
      "    self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 519, in wrapper\n",
      "    self._post_init_method(module)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1054, in _post_init_method\n",
      "    self._zero_init_param(param)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1013, in _zero_init_param\n",
      "    param.partition()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1328, in partition\n",
      "    self._partition(param_list, has_been_updated=has_been_updated)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1477, in _partition\n",
      "    self._partition_param(param, has_been_updated=has_been_updated)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1568, in _partition_param\n",
      "    param.ds_tensor.copy_(src_tensor)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/DEEPSPEED/LLaMA-Factory/src/train_bash.py\", line 14, in <module>\n",
      "    main()\n",
      "  File \"/DEEPSPEED/LLaMA-Factory/src/train_bash.py\", line 5, in main\n",
      "    run_exp()\n",
      "  File \"/DEEPSPEED/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 32, in run_exp\n",
      "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "  File \"/DEEPSPEED/LLaMA-Factory/src/llmtuner/train/sft/workflow.py\", line 33, in run_sft\n",
      "    model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)\n",
      "  File \"/DEEPSPEED/LLaMA-Factory/src/llmtuner/model/loader.py\", line 100, in load_model\n",
      "    model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, config=config, **init_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 561, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3375, in from_pretrained\n",
      "    model = cls(config, *model_args, **model_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 509, in wrapper\n",
      "    f(module, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1103, in __init__\n",
      "    self.model = LlamaModel(config)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 509, in wrapper\n",
      "    f(module, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 924, in __init__\n",
      "    [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 924, in <listcomp>\n",
      "    [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 509, in wrapper\n",
      "    f(module, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 699, in __init__\n",
      "    self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 509, in wrapper\n",
      "    f(module, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 509, in wrapper\n",
      "    f(module, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 288, in __init__\n",
      "    self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 519, in wrapper\n",
      "    self._post_init_method(module)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1054, in _post_init_method\n",
      "    self._zero_init_param(param)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1013, in _zero_init_param\n",
      "    param.partition()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1328, in partition\n",
      "    self._partition(param_list, has_been_updated=has_been_updated)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1477, in _partition\n",
      "    self._partition_param(param, has_been_updated=has_been_updated)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1568, in _partition_param\n",
      "    param.ds_tensor.copy_(src_tensor)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/deepspeed\", line 6, in <module>\n",
      "    main()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/launcher/runner.py\", line 584, in main\n",
      "    result.wait()\n",
      "  File \"/opt/conda/lib/python3.10/subprocess.py\", line 1209, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/opt/conda/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-12 07:51:15,559] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 188673\n",
      "[2024-03-12 07:51:15,984] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 188674\n",
      "[2024-03-12 07:51:16,249] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 188675\n",
      "[2024-03-12 07:51:16,556] [INFO] [launch.py:324:sigkill_handler] Main process received SIGINT, exiting\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ml libs/singularity/3.10.2\n",
    "singularity exec --nv \\\n",
    "-B $PWD:/DEEPSPEED \\\n",
    "-B /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf \\\n",
    "/work/u00cjz00/nvidia/cuda118/c00cjz00_cuda11.8_pytorch_2.1.2-cuda11.8-cudnn8-devel-llama_factory.sif \\\n",
    "bash -c '/DEEPSPEED/demo.cmd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
