{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORCx8i1o1ZK9",
    "tags": []
   },
   "source": [
    "# LLaMA-Factory webui in Singularity container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAnZLFDg1ZLC"
   },
   "source": [
    "## 初始環境設定\n",
    "- 重新reload kernel, 並切換到 kernel Image_S-work-LlamaFactory_c00cjz00_cuda11.8_pytorch_2.1.2-cuda11.8-cudnn8-devel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3UgaDyzu1ZLD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初始環境設定\n",
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 12 08:11:31 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    56W / 300W |  13896MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:B1:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    41W / 300W |      3MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     11227      C   python                          13893MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU確認\n",
    "!nvidia-smi\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 套件安裝 (此映像檔已經安裝以下套件, 若使用其他映像檔, 請移除 ## 安裝以下套件)\n",
    "##!pip install llmtuner==0.5.3 deepspeed==0.13.1  bitsandbytes==0.42.0 -q \n",
    "##!pip install deepspeed==0.13.1 -q\n",
    "##!pip install bitsandbytes==0.42.0 -q"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 下載 LLaMA-Factory 套件\n",
    "!rm -rf LLaMA-Factory\n",
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT (sft in mutiple GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-12 08:11:38,311] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-12 08:11:39,771] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-03-12 08:11:39,772] [INFO] [runner.py:568:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None src/train_bash.py --deepspeed examples/full_multi_gpu/ds_z3_config.json --stage sft --do_train --model_name_or_path /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf --dataset alpaca_gpt4_en,glaive_toolcall --dataset_dir data --template default --finetuning_type lora --lora_target q_proj,v_proj --output_dir ../saves/LLaMA2-7B/lora/sft --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 8 --lr_scheduler_type cosine --logging_steps 10 --save_steps 100 --eval_steps 100 --evaluation_strategy steps --load_best_model_at_end --learning_rate 5e-5 --num_train_epochs 3.0 --max_samples 1000 --val_size 0.1 --plot_loss --fp16\n",
      "[2024-03-12 08:11:41,612] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-12 08:11:42,491] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.2-1+cuda11.8\n",
      "[2024-03-12 08:11:42,491] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.2-1\n",
      "[2024-03-12 08:11:42,491] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.2-1\n",
      "[2024-03-12 08:11:42,491] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "[2024-03-12 08:11:42,491] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.2-1+cuda11.8\n",
      "[2024-03-12 08:11:42,491] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "[2024-03-12 08:11:42,491] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.2-1\n",
      "[2024-03-12 08:11:42,491] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}\n",
      "[2024-03-12 08:11:42,491] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0\n",
      "[2024-03-12 08:11:42,491] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n",
      "[2024-03-12 08:11:42,491] [INFO] [launch.py:163:main] dist_world_size=2\n",
      "[2024-03-12 08:11:42,491] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1\n",
      "[2024-03-12 08:11:46,289] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-12 08:11:46,297] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-03-12 08:11:49,605] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-03-12 08:11:49,605] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-03-12 08:11:49,605] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "03/12/2024 08:11:49 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "03/12/2024 08:11:49 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n",
      "03/12/2024 08:11:49 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "03/12/2024 08:11:49 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2044] 2024-03-12 08:11:49,636 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2044] 2024-03-12 08:11:49,636 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2044] 2024-03-12 08:11:49,636 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2044] 2024-03-12 08:11:49,636 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2044] 2024-03-12 08:11:49,636 >> loading file tokenizer.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/12/2024 08:11:49 - INFO - llmtuner.data.template - Add pad token: </s>\n",
      "03/12/2024 08:11:49 - INFO - llmtuner.data.loader - Loading dataset alpaca_gpt4_data_en.json...\n",
      "03/12/2024 08:11:49 - INFO - llmtuner.data.template - Add pad token: </s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting format of dataset: 100%|██████████| 1000/1000 [00:00<00:00, 27596.83 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/12/2024 08:11:50 - INFO - llmtuner.data.loader - Loading dataset glaive_toolcall_10k.json...\n",
      "03/12/2024 08:11:51 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/glaive_toolcall_10k.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting format of dataset: 100%|██████████| 1000/1000 [00:00<00:00, 20333.85 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/12/2024 08:11:59 - INFO - llmtuner.data.loader - Loading dataset alpaca_gpt4_data_en.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting format of dataset: 100%|██████████| 1000/1000 [00:00<00:00, 54542.31 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/12/2024 08:12:00 - INFO - llmtuner.data.loader - Loading dataset glaive_toolcall_10k.json...\n",
      "03/12/2024 08:12:00 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at data/glaive_toolcall_10k.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting format of dataset: 100%|██████████| 1000/1000 [00:00<00:00, 21324.22 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 2000/2000 [00:05<00:00, 394.10 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "[12968, 29901, 25538, 2211, 25562, 363, 7952, 292, 9045, 29891, 29889, 13, 7900, 22137, 29901, 29871, 29871, 29896, 29889, 382, 271, 263, 6411, 8362, 322, 18254, 768, 2738, 652, 300, 29901, 8561, 1854, 596, 592, 1338, 526, 20978, 573, 310, 263, 12875, 310, 285, 21211, 322, 18655, 1849, 29892, 20793, 26823, 29892, 3353, 2646, 1144, 29892, 322, 9045, 29891, 285, 1446, 29889, 910, 6911, 304, 3867, 596, 3573, 411, 278, 18853, 18254, 374, 1237, 304, 740, 472, 967, 1900, 322, 508, 1371, 5557, 17168, 293, 10267, 2129, 29889, 13, 13, 29906, 29889, 2201, 482, 297, 4943, 9128, 6354, 29901, 1222, 6269, 895, 338, 7618, 1455, 363, 7344, 292, 4549, 289, 2873, 29892, 2301, 7799, 29892, 322, 5881, 29875, 586, 6151, 1070, 9045, 29889, 319, 326, 363, 472, 3203, 29871, 29896, 29945, 29900, 6233, 310, 17768, 403, 14911, 711, 293, 15058, 470, 29871, 29955, 29945, 6233, 310, 14877, 20657, 15058, 1269, 4723, 29889, 13, 13, 29941, 29889, 3617, 3307, 8709, 29901, 24162, 3307, 11029, 8709, 338, 7618, 1455, 363, 9128, 322, 19119, 1532, 29899, 915, 292, 29889, 739, 6911, 304, 1072, 5987, 286, 2092, 29892, 11157, 25323, 3321, 740, 29892, 322, 11286, 9045, 29891, 14321, 322, 5198, 1540, 740, 29889, 319, 326, 363, 29871, 29955, 29899, 29929, 6199, 310, 8709, 1269, 4646, 29889, 2]\n",
      "inputs:\n",
      "Human: Give three tips for staying healthy.\n",
      "Assistant:  1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
      "\n",
      "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 29871, 29896, 29889, 382, 271, 263, 6411, 8362, 322, 18254, 768, 2738, 652, 300, 29901, 8561, 1854, 596, 592, 1338, 526, 20978, 573, 310, 263, 12875, 310, 285, 21211, 322, 18655, 1849, 29892, 20793, 26823, 29892, 3353, 2646, 1144, 29892, 322, 9045, 29891, 285, 1446, 29889, 910, 6911, 304, 3867, 596, 3573, 411, 278, 18853, 18254, 374, 1237, 304, 740, 472, 967, 1900, 322, 508, 1371, 5557, 17168, 293, 10267, 2129, 29889, 13, 13, 29906, 29889, 2201, 482, 297, 4943, 9128, 6354, 29901, 1222, 6269, 895, 338, 7618, 1455, 363, 7344, 292, 4549, 289, 2873, 29892, 2301, 7799, 29892, 322, 5881, 29875, 586, 6151, 1070, 9045, 29889, 319, 326, 363, 472, 3203, 29871, 29896, 29945, 29900, 6233, 310, 17768, 403, 14911, 711, 293, 15058, 470, 29871, 29955, 29945, 6233, 310, 14877, 20657, 15058, 1269, 4723, 29889, 13, 13, 29941, 29889, 3617, 3307, 8709, 29901, 24162, 3307, 11029, 8709, 338, 7618, 1455, 363, 9128, 322, 19119, 1532, 29899, 915, 292, 29889, 739, 6911, 304, 1072, 5987, 286, 2092, 29892, 11157, 25323, 3321, 740, 29892, 322, 11286, 9045, 29891, 14321, 322, 5198, 1540, 740, 29889, 319, 326, 363, 29871, 29955, 29899, 29929, 6199, 310, 8709, 1269, 4646, 29889, 2]\n",
      "labels:\n",
      "1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n",
      "\n",
      "2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n",
      "\n",
      "3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:726] 2024-03-12 08:12:04,195 >> loading configuration file /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-03-12 08:12:04,196 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Running tokenizer on dataset:   0%|          | 0/2000 [00:00<?, ? examples/s][INFO|modeling_utils.py:3254] 2024-03-12 08:12:04,248 >> loading weights file /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1400] 2024-03-12 08:12:04,248 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|modeling_utils.py:3363] 2024-03-12 08:12:04,248 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "[INFO|configuration_utils.py:845] 2024-03-12 08:12:04,252 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-12 08:12:04,627] [INFO] [partition_parameters.py:349:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████| 2000/2000 [00:05<00:00, 388.96 examples/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/12/2024 08:12:11 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
      "03/12/2024 08:12:11 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "03/12/2024 08:12:19 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.65s/it]\n",
      "[INFO|modeling_utils.py:3992] 2024-03-12 08:12:19,959 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4000] 2024-03-12 08:12:19,959 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:798] 2024-03-12 08:12:19,962 >> loading configuration file /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf/generation_config.json\n",
      "[INFO|configuration_utils.py:845] 2024-03-12 08:12:19,962 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/12/2024 08:12:19 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
      "03/12/2024 08:12:19 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "03/12/2024 08:12:20 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:601] 2024-03-12 08:12:20,156 >> Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-12 08:12:20,465] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.1, git-hash=unknown, git-branch=unknown\n",
      "[2024-03-12 08:12:20,480] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-03-12 08:12:20,482] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-03-12 08:12:20,482] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-03-12 08:12:20,489] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2024-03-12 08:12:20,489] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n",
      "[2024-03-12 08:12:20,489] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2024-03-12 08:12:20,489] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
      "[2024-03-12 08:12:20,687] [INFO] [utils.py:791:see_memory_usage] Stage 3 initialize beginning\n",
      "[2024-03-12 08:12:20,688] [INFO] [utils.py:792:see_memory_usage] MA 6.46 GB         Max_MA 6.94 GB         CA 6.47 GB         Max_CA 20 GB \n",
      "[2024-03-12 08:12:20,688] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 143.83 GB, percent = 19.1%\n",
      "[2024-03-12 08:12:20,691] [INFO] [stage3.py:128:__init__] Reduce bucket size 16777216\n",
      "[2024-03-12 08:12:20,692] [INFO] [stage3.py:129:__init__] Prefetch bucket size 15099494\n",
      "[2024-03-12 08:12:20,890] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2024-03-12 08:12:20,890] [INFO] [utils.py:792:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 6.47 GB         Max_CA 6 GB \n",
      "[2024-03-12 08:12:20,891] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 143.83 GB, percent = 19.1%\n",
      "Parameter Offload: Total persistent parameters: 4460544 in 193 params\n",
      "[2024-03-12 08:12:21,163] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2024-03-12 08:12:21,164] [INFO] [utils.py:792:see_memory_usage] MA 6.45 GB         Max_MA 6.46 GB         CA 6.47 GB         Max_CA 6 GB \n",
      "[2024-03-12 08:12:21,164] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 143.83 GB, percent = 19.1%\n",
      "[2024-03-12 08:12:21,370] [INFO] [utils.py:791:see_memory_usage] Before creating fp16 partitions\n",
      "[2024-03-12 08:12:21,371] [INFO] [utils.py:792:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 6.47 GB         Max_CA 6 GB \n",
      "[2024-03-12 08:12:21,371] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 143.83 GB, percent = 19.1%\n",
      "[2024-03-12 08:12:21,818] [INFO] [utils.py:791:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2024-03-12 08:12:21,819] [INFO] [utils.py:792:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 6.46 GB         Max_CA 6 GB \n",
      "[2024-03-12 08:12:21,820] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 143.85 GB, percent = 19.1%\n",
      "[2024-03-12 08:12:22,027] [INFO] [utils.py:791:see_memory_usage] Before creating fp32 partitions\n",
      "[2024-03-12 08:12:22,028] [INFO] [utils.py:792:see_memory_usage] MA 6.45 GB         Max_MA 6.45 GB         CA 6.46 GB         Max_CA 6 GB \n",
      "[2024-03-12 08:12:22,028] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 143.84 GB, percent = 19.1%\n",
      "[2024-03-12 08:12:22,237] [INFO] [utils.py:791:see_memory_usage] After creating fp32 partitions\n",
      "[2024-03-12 08:12:22,238] [INFO] [utils.py:792:see_memory_usage] MA 6.46 GB         Max_MA 6.47 GB         CA 6.47 GB         Max_CA 6 GB \n",
      "[2024-03-12 08:12:22,238] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 143.85 GB, percent = 19.1%\n",
      "[2024-03-12 08:12:22,445] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-03-12 08:12:22,445] [INFO] [utils.py:792:see_memory_usage] MA 6.46 GB         Max_MA 6.46 GB         CA 6.47 GB         Max_CA 6 GB \n",
      "[2024-03-12 08:12:22,446] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 143.85 GB, percent = 19.1%\n",
      "[2024-03-12 08:12:22,649] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-03-12 08:12:22,649] [INFO] [utils.py:792:see_memory_usage] MA 6.48 GB         Max_MA 6.49 GB         CA 6.51 GB         Max_CA 7 GB \n",
      "[2024-03-12 08:12:22,650] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 143.85 GB, percent = 19.1%\n",
      "[2024-03-12 08:12:22,650] [INFO] [stage3.py:482:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2024-03-12 08:12:22,894] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-03-12 08:12:22,895] [INFO] [utils.py:792:see_memory_usage] MA 6.51 GB         Max_MA 6.51 GB         CA 6.54 GB         Max_CA 7 GB \n",
      "[2024-03-12 08:12:22,895] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 143.85 GB, percent = 19.1%\n",
      "[2024-03-12 08:12:22,895] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n",
      "[2024-03-12 08:12:22,895] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-03-12 08:12:22,895] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-03-12 08:12:22,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]\n",
      "[2024-03-12 08:12:22,897] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x2af2e29ee1a0>\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   fp16_auto_cast ............... False\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   fp16_enabled ................. True\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-03-12 08:12:22,898] [INFO] [config.py:988:print]   global_rank .................. 0\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 8\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   graph_harvesting ............. False\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   loss_scale ................... 0\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   memory_breakdown ............. False\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   mics_shard_size .............. -1\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   optimizer_name ............... None\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   optimizer_params ............. None\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   pld_enabled .................. False\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   pld_params ................... False\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   prescale_gradients ........... False\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   scheduler_name ............... None\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   scheduler_params ............. None\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   sparse_attention ............. None\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   steps_per_print .............. inf\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   train_batch_size ............. 16\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  1\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   use_node_local_storage ....... False\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   weight_quantization_config ... None\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   world_size ................... 2\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  True\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   zero_enabled ................. True\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:988:print]   zero_optimization_stage ...... 3\n",
      "[2024-03-12 08:12:22,899] [INFO] [config.py:974:print_user_config]   json = {\n",
      "    \"train_batch_size\": 16, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 8, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 1.677722e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 1.509949e+07, \n",
      "        \"stage3_param_persistence_threshold\": 4.096000e+04, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"steps_per_print\": inf\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1812] 2024-03-12 08:12:22,899 >> ***** Running training *****\n",
      "[INFO|trainer.py:1813] 2024-03-12 08:12:22,899 >>   Num examples = 1,800\n",
      "[INFO|trainer.py:1814] 2024-03-12 08:12:22,900 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1815] 2024-03-12 08:12:22,900 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1818] 2024-03-12 08:12:22,900 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:1819] 2024-03-12 08:12:22,900 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:1820] 2024-03-12 08:12:22,900 >>   Total optimization steps = 336\n",
      "[INFO|trainer.py:1821] 2024-03-12 08:12:22,902 >>   Number of trainable parameters = 4,194,304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-12 08:12:29,013] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 13585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/336 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/subprocess.py\", line 1209, in wait\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-12 08:12:29,224] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 13585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    return self._wait(timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/opt/conda/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/deepspeed\", line 6, in <module>\n",
      "    main()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/launcher/runner.py\", line 584, in main\n",
      "    result.wait()\n",
      "  File \"/opt/conda/lib/python3.10/subprocess.py\", line 1222, in wait\n",
      "    self._wait(timeout=sigint_timeout)\n",
      "  File \"/opt/conda/lib/python3.10/subprocess.py\", line 1953, in _wait\n",
      "    time.sleep(delay)\n",
      "KeyboardInterrupt\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-12 08:12:29,264] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 13586\n",
      "[2024-03-12 08:12:29,415] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 13585\n",
      "[2024-03-12 08:12:29,415] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 13586\n",
      "[2024-03-12 08:12:29,521] [INFO] [launch.py:324:sigkill_handler] Main process received SIGINT, exiting\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## qlora in single GPU (bitsandbytes) \n",
    "## 啟動 Llama-Factory Cmd\n",
    "## 請記得修改最後一行, 給予huggingface token,  HF_TOKEN='hf_' ?\n",
    "## 記錄檔案為 llamafactory.log \n",
    "## 量化請加上     --quantization_bit 4 \\\n",
    "## --model_name_or_path /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf \\\n",
    "## meta-llama/Llama-2-7b-hf\n",
    "\n",
    "cd LLaMA-Factory\n",
    "\n",
    "HF_TOKEN='hf_' deepspeed --num_gpus 2 src/train_bash.py \\\n",
    "    --deepspeed examples/full_multi_gpu/ds_z3_config.json \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf \\\n",
    "    --dataset alpaca_gpt4_en,glaive_toolcall \\\n",
    "    --dataset_dir data \\\n",
    "    --template default \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --output_dir ../saves/LLaMA2-7B/lora/sft \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --eval_steps 100 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 1000 \\\n",
    "    --val_size 0.1 \\\n",
    "    --plot_loss \\\n",
    "    --fp16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Image_S-work-LlamaFactory_c00cjz00_cuda11.8_pytorch_2.1.2-cuda11.8-cudnn8-devel-llama_factory",
   "language": "python",
   "name": "s-work-llamafactory_c00cjz00_cuda11.8_pytorch_2.1.2-cuda11.8-cudnn8-devel-llama_factory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
